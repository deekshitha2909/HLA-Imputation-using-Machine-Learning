In this project, I explored and compared two advanced sequence-to-sequence architectures for sequence processing: a standard Long Short-Term Memory (LSTM) model and a Bidirectional LSTM (Bi-LSTM), both integrated with Bahdanau attention. This study aims to understand how these models, particularly with the inclusion of the Bahdanau attention mechanism, perform in complex sequence tasks such as machine translation. The Bahdanau attention, known for its efficacy in emphasizing relevant parts of the input sequence, complements both the LSTM’s and Bi-LSTM’s capacity to handle long-range dependencies. Through extensive experimentation on HLA genotype dataset, we assess the models’ performance in terms of BLEU score, contextual sensitivity, and computational efficiency. Our findings reveal the Bi-LSTM with Bahdanau attention outperforms its unidirectional counterpart, particularly in processing HLA sequences, underscoring the potential of bidirectional context processing and atte
